{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy\n",
        "# !pip install openai\n",
        "\n",
        "# Libraries importation\n",
        "from __future__ import annotations\n",
        "import os # to import secret variables saved on the pc\n",
        "import re # to isolate and clean the json objects from the model's response\n",
        "import json # to convert json objects in python data structures\n",
        "from dataclasses import dataclass # to define compact data classes\n",
        "from typing import List, Dict, Any, Tuple, Optional # to annotate data types\n",
        "from openai import OpenAI\n",
        "\n",
        "# API configuration (Colab)\n",
        "from google.colab import userdata  # type: ignore\n",
        "_api_key = userdata.get('API-KEY')\n",
        "client = OpenAI(api_key=_api_key)\n",
        "MODEL = \"gpt-4o-mini\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_EyaV42YSl",
        "outputId": "3f5300f0-3078-4f97-b2d0-724141c24c7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.12.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.1)\n",
            "Collecting legacy-api-wrap>=1.4.1 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.5)\n",
            "Requirement already satisfied: numba>=0.57.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (25.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.16.1)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.14.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.9.post2)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy)\n",
            "  Downloading zarr-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.1->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->scanpy) (1.17.0)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numcodecs>=0.14 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (6.0.2)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy)\n",
            "  Downloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Downloading scanpy-1.11.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.12.2-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading session_info2-0.2-py3-none-any.whl (15 kB)\n",
            "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.4/255.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: session-info2, numcodecs, legacy-api-wrap, donfig, crc32c, array-api-compat, zarr, anndata, scanpy\n",
            "Successfully installed anndata-0.12.2 array-api-compat-1.12.0 crc32c-2.7.1 donfig-0.8.1.post1 legacy-api-wrap-1.4.1 numcodecs-0.16.1 scanpy-1.11.4 session-info2-0.2 zarr-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Utilities ---------------------------------------------------------------\n",
        "def extract_json(content: str) -> Tuple[Optional[Any], Optional[Exception]]:\n",
        "    \"\"\"Robustly parse JSON from LLM outputs.\n",
        "    - Strips code fences\n",
        "    - Attempts direct json.loads\n",
        "    - Fallback: regex to first {...} or [...]\n",
        "    Returns (obj, err)\n",
        "    \"\"\"\n",
        "    if not content:\n",
        "        return None, ValueError(\"Empty content\")\n",
        "\n",
        "    # Remove single or triple backticks fences, optionally labeled as json\n",
        "    text = content.strip()\n",
        "    text = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", text, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # Direct attempt\n",
        "    try:\n",
        "        return json.loads(text), None\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Find first JSON-looking block\n",
        "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        try:\n",
        "            return json.loads(m.group(1)), None\n",
        "        except Exception as e:\n",
        "            return None, e\n",
        "\n",
        "    return None, ValueError(\"No JSON block found\")"
      ],
      "metadata": {
        "id": "D5HinTZ32Sie"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data structures ---------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Step:\n",
        "    step: int\n",
        "    title: str\n",
        "    description: str\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    step: int\n",
        "    title: str\n",
        "    description: str\n",
        "    code: str\n",
        "    justification: str"
      ],
      "metadata": {
        "id": "qTRdkn1a3CjK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts -----------------------------------------------------------------\n",
        "\n",
        "def make_planner_prompt(user_task: str, dataset_description: str) -> str:\n",
        "    # NOTE: double braces in JSON example (f-string)\n",
        "    return f\"\"\"\n",
        "Act as an expert in single-cell RNA-seq analysis. The user will provide a task.\n",
        "Your job: decompose it into an ordered list of steps (max 8). Each step must have a title and a short description.\n",
        "Be concrete and actionable. Prefer Scanpy + CellTypist in Python. Avoid R-only tools.\n",
        "\n",
        "Avoid duplicating normalization or HVG selection steps — include them once.\n",
        "\n",
        "If clusters (e.g., `leiden`) exist, suggest using them for annotation (e.g., CellTypist with group_by='leiden').\n",
        "Add a step for reviewing or verifying AnnData structure if appropriate.\n",
        "\n",
        "Dataset description: {dataset_description}\n",
        "User task: {user_task}\n",
        "\n",
        "Return ONLY a valid JSON array (no prose). Example format:\n",
        "[\n",
        "  {{\"step\": 1, \"title\": \"QC & normalization\", \"description\": \"Filter low-quality cells; normalize counts; log1p.\"}},\n",
        "  {{\"step\": 2, \"title\": \"HVG + PCA\", \"description\": \"Find highly variable genes; run PCA.\"}}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_executor_prompt(step_description: str, available_tools: List[str], context_code: str) -> str:\n",
        "    # Filter out R-only tools\n",
        "    r_only = {\"ScType\", \"Harmony\", \"Combat\", \"Slingshot\"}\n",
        "    py_tools = [t for t in available_tools if t not in r_only]\n",
        "\n",
        "    tool_docs = {\n",
        "        \"Scanpy\": (\n",
        "            \"Python library for scRNA-seq: preprocessing, HVGs, PCA/UMAP, clustering, visualization.\\n\"\n",
        "            \"Use `sc.pp.normalize_total`, `sc.pp.log1p`, `sc.pp.highly_variable_genes`, `sc.tl.pca`, `sc.pp.neighbors`, `sc.tl.leiden`, `sc.tl.umap`.\\n\"\n",
        "            \"To compute mitochondrial QC metrics, annotate mt genes with `adata.var['mt'] = adata.var_names.str.upper().str.startswith('MT-')`\\n\"\n",
        "            \"and use `sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)`.\\n\"\n",
        "            \"Use `total_counts` (not `n_counts`).\"\n",
        "            ),\n",
        "        \"CellTypist\": \"Python tool for automatic cell type annotation using pretrained models.\",\n",
        "        \"scVI\": \"Python deep generative model for batch correction and latent embedding.\",\n",
        "        \"PAGA\": \"Scanpy graph abstraction for trajectory/clustering visualization.\"\n",
        "    }\n",
        "\n",
        "    tool_info = \"\\n\".join(f\"- {t}: {tool_docs.get(t, 'N/A')}\" for t in py_tools)\n",
        "\n",
        "    # Anchor correct CellTypist usage in few-shot guidelines\n",
        "    celltypist_hint = (\n",
        "        \"\"\"\n",
        "# CellTypist reference usage (guideline):\n",
        "import celltypist\n",
        "model = celltypist.models.download_model('Immune_All_Low.pkl')\n",
        "pred = celltypist.annotate(adata, model=model, majority_voting=True, group_by='leiden')\n",
        "adata.obs['celltype'] = pred.predicted_labels\n",
        "        \"\"\".strip()\n",
        "    )\n",
        "\n",
        "    return f\"\"\"\n",
        "\n",
        "You are an expert in scRNA-seq and Python.\n",
        "Generate ONLY Python code (no R). Do not reload datasets. Use the existing variable `adata` (AnnData) and objects created by previous steps.\n",
        "Avoid redundant imports; prefer using `sc` and `adata` already in scope.\n",
        "FORBIDDEN: placeholders like 'path/to/your/data.h5ad'.\n",
        "Avoid deprecated or invalid API calls like `sc.pp.pca` — use `sc.tl.pca` instead.\n",
        "Clustering should be done using `sc.tl.leiden`, not `sc.tl.louvain`.\n",
        "\n",
        "\n",
        "Step to implement: {step_description}\n",
        "\n",
        "Available tools:\n",
        "{', '.join(py_tools) if py_tools else '(none)'}\n",
        "\n",
        "Tool notes:\n",
        "{tool_info if tool_info else '(no details)'}\n",
        "\n",
        "Previous context (you MAY reuse variables defined before):\n",
        "{context_code}\n",
        "\n",
        "Return ONLY JSON with fields \"analysis\" and \"code\":\n",
        "{{\n",
        "  \"analysis\": \"Brief explanation of what the code does and expected side-effects on adata.\",\n",
        "  \"code\": \"<executable python code that runs in this environment>\"\n",
        "}}\n",
        "\n",
        "If the step is about annotation with CellTypist, follow this usage pattern (adapt to context):\n",
        "{celltypist_hint}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_evaluator_prompt(step_description: str, code_trials: List[str], user_goal: str) -> str:\n",
        "    header = f\"\"\"\n",
        "You are an expert in scRNA-seq. Evaluate multiple Python code candidates for the step: \"{step_description}\".\n",
        "User goal: {user_goal}.\n",
        "\n",
        "Criteria (in order):\n",
        "1) Executability in Python with `adata` and `sc` in scope; no dataset reloads or undefined vars.\n",
        "2) Alignment with the step.\n",
        "3) Non-duplication and conciseness.\n",
        "4) Expected side-effects on `adata` (e.g., create 'leiden', 'celltype', embeddings) consistent with Scanpy/CellTypist APIs.\n",
        "\n",
        "Return ONLY JSON:\n",
        "{{\n",
        "  \"selected_index\": <index starting at 1>,\n",
        "  \"justification\": \"Short reason\"\n",
        "}}\n",
        "\"\"\"\n",
        "    parts = []\n",
        "    for i, code in enumerate(code_trials, 1):\n",
        "        parts.append(f\"\\n--- Code #{i} ---\\n{code}\")\n",
        "    return header + \"\\n\".join(parts)"
      ],
      "metadata": {
        "id": "f1ujmTj43Gca"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Core modules ------------------------------------------------------------\n",
        "\n",
        "def planner_task_decomposition(user_task: str, dataset_description: str = \"\") -> List[Step]:\n",
        "    prompt = make_planner_prompt(user_task, dataset_description)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=0.2,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    content = resp.choices[0].message.content\n",
        "    data, err = extract_json(content)\n",
        "    if err:\n",
        "        print(\"[Planner] JSON parse error:\", err)\n",
        "        print(\"Raw planner content:\\n\", content)\n",
        "        return []\n",
        "    steps: List[Step] = []\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, dict) and {\"step\", \"title\", \"description\"} <= set(item):\n",
        "                steps.append(Step(step=int(item[\"step\"]), title=str(item[\"title\"]), description=str(item[\"description\"])) )\n",
        "    # sort by step index just in case\n",
        "    steps.sort(key=lambda s: s.step)\n",
        "    return steps\n",
        "\n",
        "\n",
        "def executor_generate_code(step_description: str, available_tools: List[str], context_code: str = \"\") -> Tuple[str, str]:\n",
        "    prompt = make_executor_prompt(step_description, available_tools, context_code)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=0.25,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    content = resp.choices[0].message.content\n",
        "    parsed, err = extract_json(content)\n",
        "    if err or not isinstance(parsed, dict):\n",
        "        print(\"[Executor] JSON parse error:\", err)\n",
        "        print(\"Raw executor content:\\n\", content)\n",
        "        return \"\", \"\"\n",
        "    return parsed.get(\"code\", \"\"), parsed.get(\"analysis\", \"\")\n",
        "\n",
        "\n",
        "def evaluator_select_best_code(step_description: str, code_trials: List[str], user_goal: str = \"accurate result\") -> Tuple[Optional[int], str]:\n",
        "    prompt = make_evaluator_prompt(step_description, code_trials, user_goal)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=0.2,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    content = resp.choices[0].message.content\n",
        "    parsed, err = extract_json(content)\n",
        "    if err or not isinstance(parsed, dict):\n",
        "        print(\"[Evaluator] JSON parse error:\", err)\n",
        "        print(\"Raw evaluator content:\\n\", content)\n",
        "        return None, \"\"\n",
        "    idx = parsed.get(\"selected_index\")\n",
        "    just = parsed.get(\"justification\", \"\")\n",
        "    if isinstance(idx, int):\n",
        "        return idx, just\n",
        "    try:\n",
        "        return int(idx), just  # sometimes strings\n",
        "    except Exception:\n",
        "        return None, just"
      ],
      "metadata": {
        "id": "Ahc9H3Oa3Sv9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Runner / Orchestrator ---------------------------------------------------\n",
        "\n",
        "def run_cellagent_task(\n",
        "    user_task: str,\n",
        "    dataset_description: str,\n",
        "    available_tools: List[str],\n",
        "    steps_to_run: Optional[List[int]] = None,\n",
        "    trials_per_step: int = 2,\n",
        "    user_goal: str = \"accurate result\",\n",
        ") -> List[Result]:\n",
        "\n",
        "    print(\"\\nTask:\", user_task)\n",
        "    print(\"Dataset:\", dataset_description)\n",
        "    print(\"Available tools:\", \", \".join(available_tools) if available_tools else \"(none)\")\n",
        "\n",
        "    plan = planner_task_decomposition(user_task, dataset_description)\n",
        "    if not plan:\n",
        "        print(\"No plan was generated.\")\n",
        "        return []\n",
        "\n",
        "    results: List[Result] = []\n",
        "    memory_code = \"\"  # accumulate chosen code for context\n",
        "\n",
        "    for step in plan:\n",
        "        if steps_to_run and step.step not in steps_to_run:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nStep {step.step}: {step.title}\\n{step.description}\")\n",
        "\n",
        "        # Generate trials\n",
        "        trials: List[str] = []\n",
        "        for _ in range(max(1, trials_per_step)):\n",
        "            code, _analysis = executor_generate_code(step.description, available_tools, context_code=memory_code)\n",
        "            if code and code not in trials:\n",
        "                trials.append(code)\n",
        "\n",
        "        if not trials:\n",
        "            print(\"No code was generated for this step.\")\n",
        "            continue\n",
        "\n",
        "        # Evaluate\n",
        "        idx, justification = evaluator_select_best_code(step.description, trials, user_goal=user_goal)\n",
        "        if not idx or not (1 <= idx <= len(trials)):\n",
        "            # fallback to first if evaluator failed\n",
        "            idx = 1\n",
        "            if not justification:\n",
        "                justification = \"Fallback to first candidate due to evaluation parse failure.\"\n",
        "\n",
        "        best_code = trials[idx - 1]\n",
        "        print(\"Selected code:\\n\", best_code)\n",
        "        print(\"Motivation:\", justification)\n",
        "\n",
        "        results.append(Result(step=step.step, title=step.title, description=step.description, code=best_code, justification=justification))\n",
        "\n",
        "        # memory for subsequent steps\n",
        "        memory_code += f\"\\n# Step {step.step}: {step.title}\\n\" + best_code + \"\\n\"\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gz5DgMjA3UO4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example usage (comment/uncomment as needed) ----------------------------\n",
        "# Esempio di utilizzo semplice in Google Colab\n",
        "\n",
        "# 1. Carico il dataset\n",
        "filename = \"pbmc.h5ad\"\n",
        "import scanpy as sc\n",
        "adata = sc.read_h5ad(filename)\n",
        "print(f\"Dataset loaded: {adata.n_obs} cells, {adata.n_vars} genes.\")\n",
        "\n",
        "# 2. Configuro l'API OpenAI\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "api_key = userdata.get(\"API-KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"API key mancante in Colab userdata\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# 3. Definisco il task\n",
        "user_task = \"Annotate the cell types in the dataset.\"\n",
        "dataset_description = \"Human peripheral blood mononuclear cells (PBMC), single-cell RNA-seq.\"\n",
        "available_tools = [\"Scanpy\", \"CellTypist\", \"ScType\"]\n",
        "\n",
        "# 4. Eseguo la pipeline\n",
        "results = run_cellagent_task(\n",
        "    user_task=user_task,\n",
        "    dataset_description=dataset_description,\n",
        "    available_tools=available_tools,\n",
        "    steps_to_run=None,       # Esegue tutti gli step\n",
        "    trials_per_step=2,       # Due tentativi per step\n",
        "    user_goal=\"accurate result\"\n",
        ")\n",
        "\n",
        "# 5. Stampo i risultati\n",
        "print(\"\\n\\nFinal selected code per step:\")\n",
        "for r in results:\n",
        "    print(f\"\\nStep {r.step}: {r.title}\")\n",
        "    print(r.code)\n",
        "    print(\"Justification:\", r.justification)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMi3QlfR3nvk",
        "outputId": "c0d04096-ca80-4a0f-d8cb-b2d266452d5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 1087 cells, 15099 genes.\n",
            "\n",
            "Task: Annotate the cell types in the dataset.\n",
            "Dataset: Human peripheral blood mononuclear cells (PBMC), single-cell RNA-seq.\n",
            "Available tools: Scanpy, CellTypist, ScType\n",
            "\n",
            "Step 1: Load Data\n",
            "Import the PBMC single-cell RNA-seq data into an AnnData object using Scanpy.\n",
            "Selected code:\n",
            " adata.var['mt'] = adata.var_names.str.upper().str.startswith('MT-')\n",
            "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\n",
            "Motivation: Code #1 only performs quality control metrics calculation, which aligns with the initial data import step without additional processing or side effects.\n",
            "\n",
            "Step 2: QC & Normalization\n",
            "Filter low-quality cells based on metrics like total counts and gene counts; normalize counts and apply log1p transformation.\n",
            "Selected code:\n",
            " sc.pp.filter_cells(adata, min_counts=500)  # Filter cells with fewer than 500 total counts\n",
            "sc.pp.filter_cells(adata, min_genes=200)  # Filter cells with fewer than 200 genes detected\n",
            "sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize counts to a target sum of 10,000\n",
            "sc.pp.log1p(adata)  # Apply log1p transformation\n",
            "Motivation: Code #2 uses Scanpy's built-in functions for filtering, which is more aligned with best practices and ensures consistency in handling low-quality cells.\n",
            "\n",
            "Step 3: Identify Highly Variable Genes (HVG)\n",
            "Select highly variable genes to focus on for downstream analysis.\n",
            "Selected code:\n",
            " sc.pp.highly_variable_genes(adata, flavor='seurat', n_top_genes=2000, inplace=True)\n",
            "Motivation: Code #1 correctly selects highly variable genes and modifies `adata` in place without unnecessary duplication.\n",
            "\n",
            "Step 4: Dimensionality Reduction\n",
            "Perform PCA on the normalized data using the selected HVGs.\n",
            "Selected code:\n",
            " sc.tl.pca(adata, use_highly_variable=True)\n",
            "Motivation: This code directly performs PCA on the normalized data using the highly variable genes, aligning perfectly with the specified step.\n",
            "\n",
            "Step 5: Clustering\n",
            "Cluster the cells using the Leiden algorithm to identify distinct cell populations.\n",
            "Selected code:\n",
            " sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)  # Compute the neighborhood graph\n",
            "sc.tl.leiden(adata, resolution=1.0)  # Perform Leiden clustering\n",
            "Motivation: Code #1 is executable and aligns with the clustering step using the Leiden algorithm, creating the 'leiden' key in adata, while Code #2 lacks clarity on the representation used for neighbors.\n",
            "\n",
            "Step 6: Review AnnData Structure\n",
            "Check the AnnData object to ensure that the data, metadata, and clustering results are correctly structured.\n",
            "Selected code:\n",
            " print('Number of cells:', adata.n_obs)\n",
            "print('Number of genes:', adata.n_vars)\n",
            "\n",
            "# Check if highly variable genes are identified\n",
            "if 'highly_variable' in adata.var:\n",
            "    print('Number of highly variable genes:', adata.var['highly_variable'].sum())\n",
            "else:\n",
            "    print('No highly variable genes identified.')\n",
            "\n",
            "# Check clustering results\n",
            "if 'leiden' in adata.obs:\n",
            "    print('Clustering results (Leiden):')\n",
            "    print(adata.obs['leiden'].value_counts())\n",
            "else:\n",
            "    print('No clustering results found.')\n",
            "Motivation: Code #1 provides detailed checks for highly variable genes and clustering results, ensuring comprehensive evaluation of the AnnData object.\n",
            "\n",
            "Step 7: Cell Type Annotation\n",
            "Use CellTypist to annotate cell types, leveraging the clustering results with group_by='leiden'.\n",
            "Selected code:\n",
            " import celltypist\n",
            "model = celltypist.models.download_model('Immune_All_Low.pkl')\n",
            "pred = celltypist.annotate(adata, model=model, majority_voting=True, group_by='leiden')\n",
            "adata.obs['celltype'] = pred.predicted_labels\n",
            "Motivation: This code correctly uses CellTypist to annotate cell types based on the 'leiden' clustering results, directly modifies the 'adata' object by adding the 'celltype' column, and adheres to the required criteria without any undefined variables.\n",
            "\n",
            "Step 8: Review Annotations\n",
            "Examine the annotated cell types for consistency and biological relevance, making adjustments if necessary.\n",
            "Selected code:\n",
            " import pandas as pd\n",
            "\n",
            "# Review the distribution of annotated cell types\n",
            "celltype_counts = adata.obs['celltype'].value_counts()\n",
            "print('Cell type distribution:')\n",
            "print(celltype_counts)\n",
            "\n",
            "# Example adjustment: Merge similar cell types if needed\n",
            "# Here, we can define a mapping for adjustments based on biological relevance\n",
            "adjustments = {\n",
            "    'CellTypeA': 'MergedCellType1',\n",
            "    'CellTypeB': 'MergedCellType1',\n",
            "    'CellTypeC': 'MergedCellType2'\n",
            "}\n",
            "\n",
            "# Apply adjustments to the celltype annotations\n",
            "adata.obs['celltype'] = adata.obs['celltype'].replace(adjustments)\n",
            "\n",
            "# Review the updated distribution\n",
            "updated_celltype_counts = adata.obs['celltype'].value_counts()\n",
            "print('Updated cell type distribution:')\n",
            "print(updated_celltype_counts)\n",
            "Motivation: Code #1 directly examines and adjusts the annotated cell types for biological relevance, fulfilling the step's requirements.\n",
            "\n",
            "\n",
            "Final selected code per step:\n",
            "\n",
            "Step 1: Load Data\n",
            "adata.var['mt'] = adata.var_names.str.upper().str.startswith('MT-')\n",
            "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\n",
            "Justification: Code #1 only performs quality control metrics calculation, which aligns with the initial data import step without additional processing or side effects.\n",
            "\n",
            "Step 2: QC & Normalization\n",
            "sc.pp.filter_cells(adata, min_counts=500)  # Filter cells with fewer than 500 total counts\n",
            "sc.pp.filter_cells(adata, min_genes=200)  # Filter cells with fewer than 200 genes detected\n",
            "sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize counts to a target sum of 10,000\n",
            "sc.pp.log1p(adata)  # Apply log1p transformation\n",
            "Justification: Code #2 uses Scanpy's built-in functions for filtering, which is more aligned with best practices and ensures consistency in handling low-quality cells.\n",
            "\n",
            "Step 3: Identify Highly Variable Genes (HVG)\n",
            "sc.pp.highly_variable_genes(adata, flavor='seurat', n_top_genes=2000, inplace=True)\n",
            "Justification: Code #1 correctly selects highly variable genes and modifies `adata` in place without unnecessary duplication.\n",
            "\n",
            "Step 4: Dimensionality Reduction\n",
            "sc.tl.pca(adata, use_highly_variable=True)\n",
            "Justification: This code directly performs PCA on the normalized data using the highly variable genes, aligning perfectly with the specified step.\n",
            "\n",
            "Step 5: Clustering\n",
            "sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)  # Compute the neighborhood graph\n",
            "sc.tl.leiden(adata, resolution=1.0)  # Perform Leiden clustering\n",
            "Justification: Code #1 is executable and aligns with the clustering step using the Leiden algorithm, creating the 'leiden' key in adata, while Code #2 lacks clarity on the representation used for neighbors.\n",
            "\n",
            "Step 6: Review AnnData Structure\n",
            "print('Number of cells:', adata.n_obs)\n",
            "print('Number of genes:', adata.n_vars)\n",
            "\n",
            "# Check if highly variable genes are identified\n",
            "if 'highly_variable' in adata.var:\n",
            "    print('Number of highly variable genes:', adata.var['highly_variable'].sum())\n",
            "else:\n",
            "    print('No highly variable genes identified.')\n",
            "\n",
            "# Check clustering results\n",
            "if 'leiden' in adata.obs:\n",
            "    print('Clustering results (Leiden):')\n",
            "    print(adata.obs['leiden'].value_counts())\n",
            "else:\n",
            "    print('No clustering results found.')\n",
            "Justification: Code #1 provides detailed checks for highly variable genes and clustering results, ensuring comprehensive evaluation of the AnnData object.\n",
            "\n",
            "Step 7: Cell Type Annotation\n",
            "import celltypist\n",
            "model = celltypist.models.download_model('Immune_All_Low.pkl')\n",
            "pred = celltypist.annotate(adata, model=model, majority_voting=True, group_by='leiden')\n",
            "adata.obs['celltype'] = pred.predicted_labels\n",
            "Justification: This code correctly uses CellTypist to annotate cell types based on the 'leiden' clustering results, directly modifies the 'adata' object by adding the 'celltype' column, and adheres to the required criteria without any undefined variables.\n",
            "\n",
            "Step 8: Review Annotations\n",
            "import pandas as pd\n",
            "\n",
            "# Review the distribution of annotated cell types\n",
            "celltype_counts = adata.obs['celltype'].value_counts()\n",
            "print('Cell type distribution:')\n",
            "print(celltype_counts)\n",
            "\n",
            "# Example adjustment: Merge similar cell types if needed\n",
            "# Here, we can define a mapping for adjustments based on biological relevance\n",
            "adjustments = {\n",
            "    'CellTypeA': 'MergedCellType1',\n",
            "    'CellTypeB': 'MergedCellType1',\n",
            "    'CellTypeC': 'MergedCellType2'\n",
            "}\n",
            "\n",
            "# Apply adjustments to the celltype annotations\n",
            "adata.obs['celltype'] = adata.obs['celltype'].replace(adjustments)\n",
            "\n",
            "# Review the updated distribution\n",
            "updated_celltype_counts = adata.obs['celltype'].value_counts()\n",
            "print('Updated cell type distribution:')\n",
            "print(updated_celltype_counts)\n",
            "Justification: Code #1 directly examines and adjusts the annotated cell types for biological relevance, fulfilling the step's requirements.\n"
          ]
        }
      ]
    }
  ]
}
